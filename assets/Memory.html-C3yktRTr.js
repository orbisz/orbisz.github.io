import{_ as o,c as a,a as r,b as t,d as s,e as l,r as i,o as h}from"./app-ByLG9LQz.js";const c={},g={href:"https://mp.weixin.qq.com/s/SGGJWfhlYtUUieO_hl45jw",target:"_blank",rel:"noopener noreferrer"};function p(d,e){const n=i("ExternalLinkIcon");return h(),a("div",null,[e[1]||(e[1]=r('<p>你是否遇到过这样的场景：构建了一个智能 Agent，能够与用户进行多轮对话，处理复杂的任务。但随着对话的深入，你发现了一个严重的问题——</p><p><strong>对话进行到第 100 轮时，每次 API 调用需要发送 10 万 tokens，成本是初始对话的 10 倍！</strong></p><p>在长对话场景中，随着对话历史的不断积累，你会面临以下困境：</p><ul><li><strong>成本线性增长</strong>：每次 API 调用需要为所有历史 tokens 付费，成本随对话增长而线性上升；</li><li><strong>性能下降</strong>：上下文越长，模型处理时间越长，响应变慢；</li><li><strong>模型maxToken限制</strong>：当对话历史超过模型的最大上下文窗口（如 128K tokens）时，模型无法处理完整的上下文，导致请求失败；</li><li><strong>信息丢失风险</strong>：简单上下文截断会丢失关键历史信息，影响 Agent 决策质量。</li></ul><h2 id="autocontextmemory-智能上下文管理" tabindex="-1"><a class="header-anchor" href="#autocontextmemory-智能上下文管理"><span>AutoContextMemory：智能上下文管理</span></a></h2><p>AgentScope推出了AutoContextMemory，它是 AgentScope Java 框架提供的智能上下文内存管理组件， 通过自动压缩、卸载和摘要对话历史，在成本控制和信息保留之间找到最佳平衡。</p><h3 id="两大核心机制" tabindex="-1"><a class="header-anchor" href="#两大核心机制"><span>两大核心机制</span></a></h3><h4 id="_1-自动压缩与智能摘要" tabindex="-1"><a class="header-anchor" href="#_1-自动压缩与智能摘要"><span>1. 自动压缩与智能摘要</span></a></h4><ul><li>当消息或 token 数量超过阈值时，自动触发 6 种渐进式压缩策略；</li><li>使用 LLM 智能摘要，保留关键信息而非简单截断；</li><li>无需人工干预，系统自动管理</li></ul><h4 id="_2-内容卸载与完整追溯" tabindex="-1"><a class="header-anchor" href="#_2-内容卸载与完整追溯"><span>2. 内容卸载与完整追溯</span></a></h4><ul><li>将大型内容卸载到外部存储，通过 UUID 按需重载；</li><li>所有原始内容保存在原始存储中，支持完整历史追溯；</li><li>不会因为压缩而丢失任何信息。</li></ul><h3 id="autocontextmemory架构与工作原理" tabindex="-1"><a class="header-anchor" href="#autocontextmemory架构与工作原理"><span>AutoContextMemory架构与工作原理</span></a></h3><h4 id="多存储架构" tabindex="-1"><a class="header-anchor" href="#多存储架构"><span>多存储架构</span></a></h4><p>AutoContextMemory 采用多存储架构，确保在压缩的同时保留完整信息：</p><ul><li><strong>工作内存存储</strong>：存储压缩后的消息，直接参与模型推理，这是Agent实际使用的上下文</li><li><strong>原始内存存储</strong>：存储完整的、未压缩的消息历史，采用仅追加模式，支持完整历史追溯</li><li><strong>卸载上下文存储</strong>：以 UUID 为键存储卸载的消息内容，支持按需重载；</li><li><strong>压缩事件存储</strong>：记录所有压缩操作的详细信息，用于分析和优化；</li></ul><p>所有存储都支持状态持久化，可以结合<code>SessionManager</code>实现跨会话的上下文持久化。</p><h4 id="_6种渐进式开箱急用压缩策略" tabindex="-1"><a class="header-anchor" href="#_6种渐进式开箱急用压缩策略"><span>6种渐进式开箱急用压缩策略</span></a></h4><p>AutoContextMemory 的核心是 6 种渐进式压缩策略。</p><h5 id="压缩触发条件" tabindex="-1"><a class="header-anchor" href="#压缩触发条件"><span>压缩触发条件</span></a></h5><p>消息数量阈值或者Token数量阈值，两个条件满足任一即触发压缩</p><h5 id="压缩流程" tabindex="-1"><a class="header-anchor" href="#压缩流程"><span>压缩流程</span></a></h5><p>检查阈值 → 策略1(压缩历史工具调用) → 策略2(卸载大型消息-带保护)→ 策略3(卸载大型消息-无保护) → 策略4(摘要历史对话轮次)→ 策略5(摘要当前轮次大型消息) → 策略6(压缩当前轮次消息)</p><h5 id="压缩原则" tabindex="-1"><a class="header-anchor" href="#压缩原则"><span>压缩原则</span></a></h5><ul><li><strong>当前轮次优先</strong>：优先保护当前轮次的完整信息</li><li><strong>用户交互优先</strong>：用户输入和Agent回复的重要性高于工具调用的中间结果；</li><li><strong>可回溯性</strong>：所有压缩的原文都可以通过UUID回溯，确保信息不丢失；</li></ul><h5 id="_6种压缩策略" tabindex="-1"><a class="header-anchor" href="#_6种压缩策略"><span>6种压缩策略</span></a></h5><ol><li><strong>压缩历史工具调用</strong>：查找历史对话中的连续工具调用消息（超过 6 条），使用 LLM 智能压缩，保留工具名称、参数和关键结果。轻量级策略，压缩成本低。</li><li><strong>卸载大型消息（带保护）</strong>：查找超过阈值的大型消息，保护最新的助手响应和最后 N 条消息，卸载原始内容并替换为预览和 UUID 提示。快速减少 token 使用。</li><li><strong>卸载大型消息（无保护）</strong>：与策略 2 类似，但不保护最后 N 条消息，仅保护最新的助手响应。更激进的压缩策略。</li><li><strong>摘要历史对话轮次</strong>：对历史用户-助手对话对进行智能摘要，使用 LLM 生成摘要保留关键决策和信息。大幅减少 token 使用。</li><li><strong>摘要当前轮次大型消息</strong>：查找当前轮次中超过阈值的大型消息，使用 LLM 生成摘要并卸载原始内容。针对当前轮次的优化。</li><li><strong>压缩当前轮次消息</strong>：压缩当前轮次的所有消息，合并多个工具结果，保留关键信息。最后的保障策略，确保上下文不超限。</li></ol><h2 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span>参考文献</span></a></h2>',27)),t("p",null,[t("a",g,[e[0]||(e[0]=s("AgentScope AutoContextMemory：告别Agent上下文焦虑")),l(n)])])])}const x=o(c,[["render",p]]),u=JSON.parse('{"path":"/blogs/AI/Memory.html","title":"AI Agent - Context Memory","lang":"en-US","frontmatter":{"title":"AI Agent - Context Memory","date":"2025/10/05","tags":["AI","AI Agent","Context Memory"],"categories":["AI Agent 学习日志"]},"headers":[{"level":2,"title":"AutoContextMemory：智能上下文管理","slug":"autocontextmemory-智能上下文管理","link":"#autocontextmemory-智能上下文管理","children":[{"level":3,"title":"两大核心机制","slug":"两大核心机制","link":"#两大核心机制","children":[]},{"level":3,"title":"AutoContextMemory架构与工作原理","slug":"autocontextmemory架构与工作原理","link":"#autocontextmemory架构与工作原理","children":[]}]},{"level":2,"title":"参考文献","slug":"参考文献","link":"#参考文献","children":[]}],"git":{"createdTime":1766914010000,"updatedTime":1766914010000,"contributors":[{"name":"orbisz","email":"zxy0613zxy@outlook.com","commits":1}]},"filePathRelative":"blogs/AI/Memory.md"}');export{x as comp,u as data};
