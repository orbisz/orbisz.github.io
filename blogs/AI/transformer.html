<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-rc.19">
    <script>
      (function() {
        const userMode = localStorage.getItem('vuepress-reco-color-scheme') || 'auto';
        const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;

        if (userMode === 'dark' || (userMode === 'auto' && systemDarkMode)) {
          document.documentElement.classList.toggle('dark', true);
        }
      })();
    </script>
    <link rel="icon" href="/favicon1.ico"><title>Transformer 学习 | orbisz-Blog</title><meta name="description" content="Carpe diem">
    <link rel="preload" href="/assets/style-BwPacOyM.css" as="style"><link rel="stylesheet" href="/assets/style-BwPacOyM.css">
    <link rel="modulepreload" href="/assets/app-ByLG9LQz.js"><link rel="modulepreload" href="/assets/transformer.html-BZy-WUhL.js">
    <link rel="prefetch" href="/assets/timeline.html-D8bL9X10.js" as="script"><link rel="prefetch" href="/assets/posts.html-BUKh4Xkh.js" as="script"><link rel="prefetch" href="/assets/friendship-link.html-B2BltZ8v.js" as="script"><link rel="prefetch" href="/assets/1.html-By_0A9C9.js" as="script"><link rel="prefetch" href="/assets/2.html-DNr8z5qN.js" as="script"><link rel="prefetch" href="/assets/3.html-BPcxeU9p.js" as="script"><link rel="prefetch" href="/assets/1.html-3PHRKLWU.js" as="script"><link rel="prefetch" href="/assets/1.html-DkVM-LuR.js" as="script"><link rel="prefetch" href="/assets/1.html-NS5LR8hY.js" as="script"><link rel="prefetch" href="/assets/1.html-C63klMuD.js" as="script"><link rel="prefetch" href="/assets/2.html-CPgYeXMJ.js" as="script"><link rel="prefetch" href="/assets/1.html-mMC7k61w.js" as="script"><link rel="prefetch" href="/assets/2.html-CJMtKSph.js" as="script"><link rel="prefetch" href="/assets/1.html-DBlSI2e4.js" as="script"><link rel="prefetch" href="/assets/1.html-DCSLntsV.js" as="script"><link rel="prefetch" href="/assets/1.html-DTLpyGl-.js" as="script"><link rel="prefetch" href="/assets/2.html-8A1WoTob.js" as="script"><link rel="prefetch" href="/assets/1.html-Bhsd_8Ma.js" as="script"><link rel="prefetch" href="/assets/1.html-BcEZH6QN.js" as="script"><link rel="prefetch" href="/assets/1.html-DUo9XIu7.js" as="script"><link rel="prefetch" href="/assets/1.html-aeCs_6R3.js" as="script"><link rel="prefetch" href="/assets/1.html-CLsRxVZA.js" as="script"><link rel="prefetch" href="/assets/1.html-CPflLrjL.js" as="script"><link rel="prefetch" href="/assets/1.html-BZQF61Kr.js" as="script"><link rel="prefetch" href="/assets/1.html-BuQ4vSn_.js" as="script"><link rel="prefetch" href="/assets/1.html-_RYSIzBN.js" as="script"><link rel="prefetch" href="/assets/1.html-vSegm2e1.js" as="script"><link rel="prefetch" href="/assets/1.html-C3gqmBSn.js" as="script"><link rel="prefetch" href="/assets/1.html-CsjAdv5i.js" as="script"><link rel="prefetch" href="/assets/1.html-ClKiFCJy.js" as="script"><link rel="prefetch" href="/assets/1.html-oL1sf9fB.js" as="script"><link rel="prefetch" href="/assets/1.html-BI_bNO4t.js" as="script"><link rel="prefetch" href="/assets/1.html-CqNXE2Q5.js" as="script"><link rel="prefetch" href="/assets/1.html-k1hb51ft.js" as="script"><link rel="prefetch" href="/assets/1.html-Y0qfODzV.js" as="script"><link rel="prefetch" href="/assets/1.html-CCOmfO_W.js" as="script"><link rel="prefetch" href="/assets/2.html-D9xEUk1-.js" as="script"><link rel="prefetch" href="/assets/1.html-BpKCwn5L.js" as="script"><link rel="prefetch" href="/assets/1.html-nkyid-Hd.js" as="script"><link rel="prefetch" href="/assets/1.html-Bmy6fa2j.js" as="script"><link rel="prefetch" href="/assets/1.html-COJM2PfY.js" as="script"><link rel="prefetch" href="/assets/1.html-C8ReidTp.js" as="script"><link rel="prefetch" href="/assets/2.html-TpnPJT_m.js" as="script"><link rel="prefetch" href="/assets/1.html-B3SgaIFv.js" as="script"><link rel="prefetch" href="/assets/1.html-CLI4rAg1.js" as="script"><link rel="prefetch" href="/assets/1.html-2u3e_Su1.js" as="script"><link rel="prefetch" href="/assets/1.html-DPL_yd3D.js" as="script"><link rel="prefetch" href="/assets/2.html-DbERg0Rh.js" as="script"><link rel="prefetch" href="/assets/1.html-CATswM7G.js" as="script"><link rel="prefetch" href="/assets/1.html-BHKdEpx2.js" as="script"><link rel="prefetch" href="/assets/1.html-3kCVGkdp.js" as="script"><link rel="prefetch" href="/assets/1.html-C5G71N1C.js" as="script"><link rel="prefetch" href="/assets/1.html-WVycvXvq.js" as="script"><link rel="prefetch" href="/assets/1.html-Bmy6fa2j.js" as="script"><link rel="prefetch" href="/assets/1.html-BaRIUcjq.js" as="script"><link rel="prefetch" href="/assets/1.html-riVy44zH.js" as="script"><link rel="prefetch" href="/assets/1.html-DoIiqL-S.js" as="script"><link rel="prefetch" href="/assets/1.html-rulaLow8.js" as="script"><link rel="prefetch" href="/assets/1.html-BaRIUcjq.js" as="script"><link rel="prefetch" href="/assets/1.html-DmxGrvk8.js" as="script"><link rel="prefetch" href="/assets/1.html-C0l0Jkzg.js" as="script"><link rel="prefetch" href="/assets/1.html-GBpwaCie.js" as="script"><link rel="prefetch" href="/assets/1.html-_82cBIMV.js" as="script"><link rel="prefetch" href="/assets/1.html-CWfgrjaY.js" as="script"><link rel="prefetch" href="/assets/1.html-CtezQova.js" as="script"><link rel="prefetch" href="/assets/1.html-Dd5NbL6R.js" as="script"><link rel="prefetch" href="/assets/1.html-B0V40mYZ.js" as="script"><link rel="prefetch" href="/assets/1.html-Coqjh3BA.js" as="script"><link rel="prefetch" href="/assets/1.html-pZpEnwGj.js" as="script"><link rel="prefetch" href="/assets/1.html-DQjz_ldM.js" as="script"><link rel="prefetch" href="/assets/1.html-UzeOju-U.js" as="script"><link rel="prefetch" href="/assets/1.html-DU1MhTdD.js" as="script"><link rel="prefetch" href="/assets/1.html-CYmx0txs.js" as="script"><link rel="prefetch" href="/assets/2.html-BJBEBjEZ.js" as="script"><link rel="prefetch" href="/assets/3.html-C6CfWmfx.js" as="script"><link rel="prefetch" href="/assets/4.html-Ukw6i0YW.js" as="script"><link rel="prefetch" href="/assets/5.html-CAZt63lh.js" as="script"><link rel="prefetch" href="/assets/6.html-DK4tGfxS.js" as="script"><link rel="prefetch" href="/assets/index.html-BYz3YNfS.js" as="script"><link rel="prefetch" href="/assets/message-board.html-BcDI9Zkh.js" as="script"><link rel="prefetch" href="/assets/1.html-B29hZLBO.js" as="script"><link rel="prefetch" href="/assets/2.html-CrQNYLf5.js" as="script"><link rel="prefetch" href="/assets/AI_Coding.html-BtfP7Fop.js" as="script"><link rel="prefetch" href="/assets/claude.html-B4XkvgU5.js" as="script"><link rel="prefetch" href="/assets/MCP.html-C5AAcB-0.js" as="script"><link rel="prefetch" href="/assets/Memory.html-C3yktRTr.js" as="script"><link rel="prefetch" href="/assets/paper1.html-_YcSspz0.js" as="script"><link rel="prefetch" href="/assets/Prompt.html-CvIZrWLt.js" as="script"><link rel="prefetch" href="/assets/RAG.html-Dvt9_VvF.js" as="script"><link rel="prefetch" href="/assets/DevelopmentTips.html-sPVXWJ6z.js" as="script"><link rel="prefetch" href="/assets/DI.html-BBUdts9f.js" as="script"><link rel="prefetch" href="/assets/MVCC.html-DPWGTGXx.js" as="script"><link rel="prefetch" href="/assets/mysql.html-B-1wD3tc.js" as="script"><link rel="prefetch" href="/assets/RDBandAOF.html-BrbE_LMN.js" as="script"><link rel="prefetch" href="/assets/SpringBootStarter.html-v-qKlnoz.js" as="script"><link rel="prefetch" href="/assets/xitongshejitu.html-Bof_9_b0.js" as="script"><link rel="prefetch" href="/assets/yuanma.html-BCQPodmS.js" as="script"><link rel="prefetch" href="/assets/1.html-Br9DSheP.js" as="script"><link rel="prefetch" href="/assets/1.html-CcP5DXmW.js" as="script"><link rel="prefetch" href="/assets/2.html-Cm4dgxaI.js" as="script"><link rel="prefetch" href="/assets/3.html-w8dg-vrR.js" as="script"><link rel="prefetch" href="/assets/5.html-Bcef1yeB.js" as="script"><link rel="prefetch" href="/assets/graph.html-CMWK02fE.js" as="script"><link rel="prefetch" href="/assets/offer-java.html-DaXG1nfx.js" as="script"><link rel="prefetch" href="/assets/offer-java2.html-C6oKnrQY.js" as="script"><link rel="prefetch" href="/assets/offer-java3.html-tlhilY5a.js" as="script"><link rel="prefetch" href="/assets/paixu.html-BhIgkN9d.js" as="script"><link rel="prefetch" href="/assets/stackandheap.html-BAsHdm8E.js" as="script"><link rel="prefetch" href="/assets/tanxin.html-Bk3RTeJQ.js" as="script"><link rel="prefetch" href="/assets/union.html-BYK06_pi.js" as="script"><link rel="prefetch" href="/assets/2.html-BDRLyHub.js" as="script"><link rel="prefetch" href="/assets/OpenAI2025.html-C4v3A5bQ.js" as="script"><link rel="prefetch" href="/assets/DDD.html-DP-1YKBE.js" as="script"><link rel="prefetch" href="/assets/1.html-s1SVgBsH.js" as="script"><link rel="prefetch" href="/assets/docker.html-B3AxyQoO.js" as="script"><link rel="prefetch" href="/assets/mybatis.html-DsKD7sz4.js" as="script"><link rel="prefetch" href="/assets/1.html-4OfMoKIH.js" as="script"><link rel="prefetch" href="/assets/Netty.html-CvNvs8gj.js" as="script"><link rel="prefetch" href="/assets/code-review.html-0blJbOTz.js" as="script"><link rel="prefetch" href="/assets/1.html-C3we6iIC.js" as="script"><link rel="prefetch" href="/assets/IM.html-Dmny0O1g.js" as="script"><link rel="prefetch" href="/assets/mybatis.html-Ckxkw3mS.js" as="script"><link rel="prefetch" href="/assets/one-api-sdk.html-C554EwZe.js" as="script"><link rel="prefetch" href="/assets/dayingxiao.html-CvzF91LB.js" as="script"><link rel="prefetch" href="/assets/dayingxiao1.html-BkE9kCui.js" as="script"><link rel="prefetch" href="/assets/dayingxiao2.html-C7nq8KqK.js" as="script"><link rel="prefetch" href="/assets/dayingxiao3.html-BrwNtLNp.js" as="script"><link rel="prefetch" href="/assets/dayingxiao4.html-BFTeAFH7.js" as="script"><link rel="prefetch" href="/assets/guide.html-1rgatwtb.js" as="script"><link rel="prefetch" href="/assets/1.html-KhhNp96o.js" as="script"><link rel="prefetch" href="/assets/2.html-BdjVFHml.js" as="script"><link rel="prefetch" href="/assets/3.html-DL9V897o.js" as="script"><link rel="prefetch" href="/assets/4.html-BNXZBfjk.js" as="script"><link rel="prefetch" href="/assets/4.html-Cxyn_WRa.js" as="script"><link rel="prefetch" href="/assets/5.html-skRFr1UF.js" as="script"><link rel="prefetch" href="/assets/wrench1.html-B0D5O5uC.js" as="script"><link rel="prefetch" href="/assets/wrench2.html-BG3zwCYI.js" as="script"><link rel="prefetch" href="/assets/wrench3.html-CGUz_RsK.js" as="script"><link rel="prefetch" href="/assets/404.html-Dj_9d147.js" as="script"><link rel="prefetch" href="/assets/Valine.min-_LyT3bfY.js" as="script"><link rel="prefetch" href="/assets/giscus-aTimukGI-DWEKOTfS.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container series--no show-catalog"><header class="navbar-container not-open"><div class="navbar-inner"><div class="site-brand nav-item"><img class="logo" src="/logo1.jpg" alt="orbisz-Blog"><a href="/" class="site-name can-hide">orbisz-Blog</a></div><div class="nav-item navbar-links-wrapper" style=""><div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form></div><nav class="navbar-links"><!--[--><div class="navbar-links__item"><a href="/" class="link" aria-label="首页"><!--[--><!--]--><span class="xicon-container left"><!--[--><IconHome class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"></IconHome><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->首页<!--]--></span></span><!--[--><!--]--></a></div><div class="navbar-links__item"><a href="/docs/message-board" class="link" aria-label="留言板"><!--[--><!--]--><span class="xicon-container left"><!--[--><IconChat class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"></IconChat><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->留言板<!--]--></span></span><!--[--><!--]--></a></div><div class="navbar-links__item"><a class="link" href="https://blog.csdn.net/hywzxy" target="_blank" rel="noopener noreferrer" aria-label="CSDN"><!--[--><!--]--><span class="xicon-container left"><!--[--><IconCSDN class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"></IconCSDN><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->CSDN<!--]--></span></span><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><div class="navbar-links__item"><a class="link" href="https://github.com/orbisz" target="_blank" rel="noopener noreferrer" aria-label="Github"><!--[--><!--]--><span class="xicon-container left"><!--[--><IconGithub class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"></IconGithub><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Github<!--]--></span></span><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></div><div class="navbar-links__item"><div class="dropdown-link"><button class="dropdown-link__title" type="button" aria-label="项目体验"><span class="xicon-container left title"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->项目体验<!--]--></span></span><span class="arrow down"></span></button><button class="dropdown-link--mobile__title" type="button" aria-label="项目体验"><span class="title"><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->项目体验<!--]--></span></span></span><span class="right arrow"></span></button><ul style="display:none;" class="dropdown-link__container"><!--[--><li class="dropdown-link__item"><a class="link" href="http://117.72.164.204:3000/?userId=zxy&amp;activityId=100301" target="_blank" rel="noopener noreferrer" aria-label="幸运营销汇"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->幸运营销汇<!--]--></span></span><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a class="link" href="http://101.43.191.204" target="_blank" rel="noopener noreferrer" aria-label="Ai-Agent 智能体 "><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Ai-Agent 智能体 <!--]--></span></span><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!--[--><!--]--></a></li><!--]--></ul></div></div><!--]--><!----><span class="xicon-container btn-toggle-dark-mode btn--dark-mode navbar-links__item"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" style="width:20px;height:20px;font-size:20px;color:;"><path d="M15 2h2v3h-2z" fill="currentColor"></path><path d="M27 15h3v2h-3z" fill="currentColor"></path><path d="M15 27h2v3h-2z" fill="currentColor"></path><path d="M2 15h3v2H2z" fill="currentColor"></path><path d="M5.45 6.884l1.414-1.415l2.121 2.122l-1.414 1.414z" fill="currentColor"></path><path d="M23 7.58l2.121-2.12l1.414 1.414l-2.121 2.121z" fill="currentColor"></path><path d="M23.002 24.416l1.415-1.414l2.12 2.122l-1.413 1.414z" fill="currentColor"></path><path d="M5.47 25.13L7.59 23L9 24.42l-2.12 2.12l-1.41-1.41z" fill="currentColor"></path><path d="M16 8a8 8 0 1 0 8 8a8 8 0 0 0-8-8zm0 14a6 6 0 0 1 0-12z" fill="currentColor"></path></svg></span><ul class="social-links navbar-links__item"><!--[--><!--]--></ul></nav><span class="xicon-container btn-toggle-menus"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" style="width:20px;height:20px;font-size:20px;color:;"><circle cx="16" cy="8" r="2" fill="currentColor"></circle><circle cx="16" cy="16" r="2" fill="currentColor"></circle><circle cx="16" cy="24" r="2" fill="currentColor"></circle></svg></span></div></div></header><!----><!----><!----><div class="theme-main" style=""><!----><!--[--><main class="page-container"><div class="page-content"><h1 class="page-title">Transformer 学习</h1><div class="page-info"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M16 4a5 5 0 1 1-5 5a5 5 0 0 1 5-5m0-2a7 7 0 1 0 7 7a7 7 0 0 0-7-7z" fill="currentColor"></path><path d="M26 30h-2v-5a5 5 0 0 0-5-5h-6a5 5 0 0 0-5 5v5H6v-5a7 7 0 0 1 7-7h6a7 7 0 0 1 7 7z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->orbisz<!--]--></span></span><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M26 4h-4V2h-2v2h-8V2h-2v2H6c-1.1 0-2 .9-2 2v20c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 22H6V12h20v14zm0-16H6V6h4v2h2V6h8v2h2V6h4v4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2025/10/28<!--]--></span></span><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M11.17 6l3.42 3.41l.58.59H28v16H4V6h7.17m0-2H4a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h24a2 2 0 0 0 2-2V10a2 2 0 0 0-2-2H16l-3.41-3.41A2 2 0 0 0 11.17 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[--><!--[--><a href="/categories/AI-Agent-xuexirizhi/1.html" class="">AI Agent 学习日志</a><!--]--><!--]--></span></span><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M10 14a4 4 0 1 1 4-4a4.005 4.005 0 0 1-4 4zm0-6a2 2 0 1 0 1.998 2.004A2.002 2.002 0 0 0 10 8z" fill="currentColor"></path><path d="M16.644 29.415L2.586 15.354A2 2 0 0 1 2 13.941V4a2 2 0 0 1 2-2h9.941a2 2 0 0 1 1.414.586l14.06 14.058a2 2 0 0 1 0 2.828l-9.943 9.943a2 2 0 0 1-2.829 0zM4 4v9.942L18.058 28L28 18.058L13.942 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[--><!--[--><a href="/tags/AI/1.html" class="">AI</a><a href="/tags/Transformer/1.html" class="">Transformer</a><!--]--><!--]--></span></span><span class="xicon-container left"><!--[--><svg class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;font-size:18px;" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 12 12"><g fill="none"><path d="M1.974 6.659a.5.5 0 0 1-.948-.317c-.01.03 0-.001 0-.001a1.633 1.633 0 0 1 .062-.162c.04-.095.099-.226.18-.381c.165-.31.422-.723.801-1.136C2.834 3.827 4.087 3 6 3c1.913 0 3.166.827 3.931 1.662a5.479 5.479 0 0 1 .98 1.517l.046.113c.003.008.013.06.023.11L11 6.5s.084.333-.342.474a.5.5 0 0 1-.632-.314v-.003l-.006-.016a3.678 3.678 0 0 0-.172-.376a4.477 4.477 0 0 0-.654-.927C8.584 4.673 7.587 4 6 4s-2.584.673-3.194 1.338a4.477 4.477 0 0 0-.795 1.225a2.209 2.209 0 0 0-.03.078l-.007.018zM6 5a2 2 0 1 0 0 4a2 2 0 0 0 0-4zM5 7a1 1 0 1 1 2 0a1 1 0 0 1-2 0z" fill="currentColor"></path></g></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[--><span id="/blogs/AI/transformer.html" class="leancloud-visitors" data-flag-title="Your Article Title"><a class="leancloud-visitors-count" style=""></a></span><!----><!--]--></span></span></div><div class="theme-reco-md-content"><div><p>Transformer 是第一个完全依赖自注意力（self-attention）来计算输入和输出的表示，而不使用序列对齐的递归神经网络或卷积神经网络的转换模型。</p><blockquote><p>自注意力（Self-Attention）是Transformer 模型的核心机制，它让模型能在处理序列（如文本、语音）时，动态捕捉 “序列中每个元素与其他所有元素的依赖关系”—— 简单说，就是让模型 “知道在当前语境下，哪些词更重要，需要重点关注”。</p><p>核心是通过 “Q（查询）、K（键）、V（值）” 的计算，生成 “注意力权重”，再加权求和得到最终输出。</p></blockquote><h2 id="输入编码-bpe-pe-剖析" tabindex="-1"><a class="header-anchor" href="#输入编码-bpe-pe-剖析"><span>输入编码(BPE,PE)剖析</span></a></h2><p>进入Pre-Train时代后模型处理文本的粒度从Word更细分到Token粒度，Token可以是一个字、词、标识符等等。</p><p>Tokenizer——分词器，可以将文本处理成Token的序列，例如当BertTokenizer的输入文本是 &quot;I love NLP.&quot; 会被切分为：[&#39;i&#39;, &#39;love&#39;, &#39;n&#39;, &#39;##lp&#39;, &#39;.&#39;]； 当输入文本是: &quot;我喜欢自然语言处理&quot;时， 则会被切分为：[&#39;我&#39;, &#39;喜&#39;, &#39;欢&#39;, &#39;自&#39;, &#39;然&#39;, &#39;语&#39;, &#39;言&#39;, &#39;处&#39;, &#39;理&#39;, &#39;。&#39;]</p><p>输出的token序列是根据Tokenizer中已经生成好的词表进行匹配（过程类似最大前向匹配），匹配过程中如果词表中存在该token就直接输出，没有就输出特殊符号[UNK]。</p><p>因此本质上是Tokenizer中的词表决定生成文本切词方式，而决定Tokenizer中词表生成则是由不同的分词算法所决定。</p><h3 id="bpe分词算法" tabindex="-1"><a class="header-anchor" href="#bpe分词算法"><span>BPE分词算法</span></a></h3><p>BPE (Byte Pair Encoding)全称为字节对编码，本质是一种数据压缩方法，来自于一篇发表于1994年的论文：“A new algorithm for data compression”。 核心逻辑是 “从最小的字符单元（字节）开始，通过反复合并高频出现的字节对，生成粒度更大的子词”。比如有一段数据是“abbabyabya”，其中相邻字母对的组合中&quot;ab&quot;出现3次是出现最多， 即可以使用一个字母Z去代替&quot;ab&quot;，数据形式压缩成：&quot;ZbZyZya&quot;。以此类推，下一个&#39;Zy&#39;继续被替换成Y，数据形式变成：&quot;ZbYYa&quot;。</p><p><strong>BPE核心价值</strong>：通过“子词（Subword）”平衡词汇量和语义颗粒度之间的矛盾——子词是 “字符与单词之间的中间粒度”（如 “low”“er”“est”），既能控制词汇量（子词数量远少于单词），又能处理 OOV（未见过的词可拆为已学子词，如 “lowest” 拆为 “low”+“est”），同时保留一定语义颗粒度。</p><h4 id="算法步骤" tabindex="-1"><a class="header-anchor" href="#算法步骤"><span>算法步骤</span></a></h4><ol><li>准备足够大的训练语料，确定期望的subword词表大小；</li><li>准备基础词表：比如英文中26个字母加上各种符号；</li><li>基于基础词表将语料中的单词拆分为字符序列并在末尾添加后缀“ &lt;/ w&gt;”；本阶段的subword的粒度是字符。例如单词“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5；</li><li>统计每一个连续字节对的出现频率，选择最高频的字符对合并成新的subword；</li><li>重复第4步直到达到第1步设定的subword词表大小或下一个最高频的字节对出现频率为1</li><li>当合并次数达到预设值（或词汇量达标），停止合并，此时所有 “独立单元”（字符 + 合并后的子词）构成 BPE 的子词词汇表</li><li>当遇到未见过的词（如lowest），BPE 会按 “最长匹配原则” 拆分为已有的子词</li></ol><p>BPE 的核心是 “从字节开始，合并高频对，生成子词”，它用数据驱动的方式解决了传统分词的 “词汇量爆炸” 和 “OOV” 痛点，成为 LLM 处理文本的 “标配前置步骤”。</p><h3 id="pe位置编码" tabindex="-1"><a class="header-anchor" href="#pe位置编码"><span>PE位置编码</span></a></h3><p>PE（Positional Encoding，位置编码）是为 Transformer 模型设计的 “序列顺序信息注入机制”—— 由于 Transformer 的核心组件（多头注意力）是 “并行处理所有 token” 的（无 RNN 的循环顺序、无 CNN 的局部窗口）， 天然无法捕捉 “谁在前、谁在后” 的序列位置关系，而 PE 的作用就是通过数学方式，给每个 token 的嵌入向量（Embedding）附加 “位置信息”，让模型能区分 “我吃苹果” 和 “苹果吃我” 这类语义完全相反的序列。</p><table><thead><tr><th>模型类型</th><th>序列顺序捕捉方式</th><th>问题/优势</th></tr></thead><tbody><tr><td>RNN/LSTM</td><td>循环处理：按“第1个token→第2个token→…→第n个token”的顺序计算，每个token的状态依赖前一个token</td><td><strong>优势</strong>：天然包含位置信息；<br><strong>问题</strong>：并行效率低，长序列易遗忘</td></tr><tr><td>Transformer</td><td>并行处理：所有token同时输入，通过注意力权重计算相互依赖关系，无固定处理顺序</td><td><strong>优势</strong>：并行效率高，长序列依赖捕捉能力强；<br><strong>问题</strong>：丢失位置信息——模型不知道“token A在token B前面”还是后面</td></tr></tbody></table><blockquote><p>举个例子</p><p>对于序列 “我 吃 苹果” 和 “苹果 吃 我”，Transformer 的注意力层会计算 “我” 与 “吃”“苹果” 的依赖关系，但如果没有 PE，这两个序列的 token 嵌入向量完全相同， 模型无法区分 “谁是主语、谁是宾语”，最终输出语义混乱。</p><p>而 PE 的作用就是给这两个序列的每个 token “打位置标签”：比如 “我” 在第 1 位和第 3 位的 PE 不同，“苹果” 在第 3 位和第 1 位的 PE 不同，从而让模型能识别序列顺序差异。</p></blockquote><p>*<em>Transformer原论文（2017）提出了PE的4个核心设计原则，这也是所有PE变体（如RoPE、Learned PE）的设计依据</em>：</p><ol><li><strong>与嵌入向量兼容</strong>：PE的维度必须和token嵌入向量的维度（<code>d_model</code>）完全一致，才能直接与嵌入向量“相加”（而非拼接，避免增加模型复杂度）；</li><li><strong>支持任意序列长度</strong>：PE必须能生成“任意长度”的位置信息（比如训练时处理512长度的序列，推理时处理1024长度的序列，PE仍能有效工作）；</li><li><strong>捕捉相对位置</strong>：模型需要知道“token A和token B之间的距离”（比如A在B前面3个位置），而非仅知道“A在第5位、B在第8位”的绝对位置——即PE需满足“相对位置不变性”（如位置<code>pos</code>和<code>pos+k</code>的相对关系，与<code>pos</code>无关）；</li><li><strong>计算高效</strong>：PE的生成过程不能太复杂（如避免迭代计算），否则会抵消Transformer的并行效率优势。</li></ol><p>Transformer原论文采用<strong>正弦（sin）和余弦（cos）函数</strong>设计PE，是最经典、应用最广的方案。 <img src="/assets/img_12-BEl_JqFp.png" alt="img_12.png"></p><p>PE的使用非常简单，只需将“token的嵌入向量（Embedding）”与“该token的PE向量”直接相加，再输入到Transformer的多头注意力层：<br> $$ \text{Input} = \text{Embedding}(token) + \text{PE}(pos) $$</p><ul><li>为什么是“相加”而非“拼接”？<br> 拼接会导致输入维度变为<code>d_model + d_model = 2d_model</code>，增加模型参数和计算量；而相加能在不增加维度的前提下，将位置信息融入token表示，且符合“内容信息与位置信息融合”的直觉（token的含义与其位置相关）。</li></ul><h4 id="pe的常见变体" tabindex="-1"><a class="header-anchor" href="#pe的常见变体"><span>PE的常见变体</span></a></h4><p>随着大模型的发展，研究者提出了多种PE变体，以解决不同场景的需求（如长序列、多语言、效率优化），核心仍是“注入位置信息”，但实现方式不同：</p><table><thead><tr><th>PE类型</th><th>核心思路</th><th>优势</th><th>缺点</th><th>典型应用场景</th></tr></thead><tbody><tr><td>可学习位置编码（Learned PE）</td><td>不使用固定公式，而是随机初始化一个<code>(max_seq_len, d_model)</code>的PE矩阵，通过训练学习最优的位置信息</td><td>灵活，能适配特定任务（如多语言、代码）</td><td>泛化性差（训练时<code>max_seq_len=512</code>，推理时处理1024长度序列需插值）</td><td>BERT（早期版本）、T5</td></tr><tr><td>旋转位置编码（RoPE）</td><td>将位置信息编码为“旋转矩阵”，通过复数乘法将位置信息注入token的嵌入向量（而非直接相加）</td><td>长序列处理能力强，相对位置表达更精准</td><td>计算需复数运算，实现稍复杂</td><td>LLaMA、ChatGLM、Qwen</td></tr><tr><td>相对位置编码（Relative PE）</td><td>不给每个token分配绝对PE，而是在注意力计算时直接建模“两个token的相对距离”（如用距离嵌入表）</td><td>完全聚焦相对位置，避免绝对位置偏见</td><td>注意力计算复杂度增加（需额外处理距离）</td><td>Transformer-XL、DeBERTa</td></tr><tr><td>ALiBi</td><td>不给token注入PE，而是在注意力权重计算时，给“距离越远的token对”附加一个线性衰减的偏置</td><td>无需PE矩阵，节省内存，支持任意长序列</td><td>偏置设计依赖经验，部分任务效果略差</td><td>PaLM、LLaMA-2（部分）</td></tr></tbody></table><p>PE的本质是<strong>给Transformer模型“补充序列顺序信息的数学工具”</strong>——它解决了Transformer并行计算导致的“位置遗忘”问题，让模型能理解“token的先后关系”。经典的正弦余弦PE通过三角函数的特性，实现了“任意长度适配、相对位置捕捉、计算高效”的目标，而RoPE等变体则在长序列、精准性等场景下做了优化。理解PE的关键是记住：它不是“额外的负担”，而是Transformer能处理序列任务的“核心前提”。</p><h3 id="input" tabindex="-1"><a class="header-anchor" href="#input"><span>Input</span></a></h3><p><img src="/assets/img_13-Cx2c6NEM.png" alt="img_13.png"></p><ul><li>文本通过BPE 分词得到子词序列，每个子词被映射为d维的嵌入向量（Input Embedding）；</li><li>给每个嵌入向量叠加对应的位置编码（Positional Encoding）；</li><li>最终的 “BPE + PE” 向量作为模型的输入，进入 Transformer 的多头注意力层。</li></ul><h2 id="transformer网络结构" tabindex="-1"><a class="header-anchor" href="#transformer网络结构"><span>Transformer网络结构</span></a></h2><p><img src="/assets/img_14-Brexfchd.png" alt="img_14.png"></p><h3 id="嵌入表示层以及python实现" tabindex="-1"><a class="header-anchor" href="#嵌入表示层以及python实现"><span>嵌入表示层以及Python实现</span></a></h3><p>对于输入文本序列，首先通过输入嵌入层（Input Embedding）将每个单词转换为其相对应的向量表示。通常直接对每个单词创建一个向量表示。 由于Transfomer模型不再使用基于循环的方式建模文本输入，序列中不再有任何信息能够提示模型单词之间的相对位置关系。 在送入编码器端建模其上下文语义之前，一个非常重要的操作是在词嵌入中加入位置编码（Positional Encoding）这一特征。 具体来说，序列中每一个单词所在的位置都对应一个向量。这一向量会与单词表示对应相加并送入到后续模块中做进一步处理。在训练的过程当中，模型会自动地学习到如何利用这部分位置信息。</p><p>Transformer模型通过偶数位置用正弦函数，奇数位置用余弦函数计算位置编码。这样就有两个好处：</p><ul><li>正余弦函数的范围是在[-1,+1]，导出的位置编码与原词嵌入相加不会使得结果偏离过远而破坏原有单词的语义信息。</li><li>依据三角函数的基本性质，可以得知第pos+k个位置的编码是第pos个位置的编码的线性组合，这就意味着位置编码中蕴含着单词之间的距离信息。</li></ul><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">class</span> <span class="token class-name">PositionalEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> max_seq_len<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model</span>
<span class="line">        <span class="token comment"># 根据 pos 和 i 创建一个常量 PE 矩阵</span></span>
<span class="line">        pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_seq_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">for</span> pos <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_seq_len<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">                pe<span class="token punctuation">[</span>pos<span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> math<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>pos <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">10000</span><span class="token operator">**</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> i<span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">                pe<span class="token punctuation">[</span>pos<span class="token punctuation">,</span></span>
<span class="line">                   i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> math<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>pos <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">10000</span><span class="token operator">**</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        pe <span class="token operator">=</span> pe<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">&#39;pe&#39;</span><span class="token punctuation">,</span> pe<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token comment"># 使得单词嵌入表示相对大一些</span></span>
<span class="line">        x <span class="token operator">=</span> x <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># 增加位置常量到单词嵌入表示中</span></span>
<span class="line">        seq_len <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> x <span class="token operator">+</span> Variable<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_len<span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">return</span> x</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="注意力层及实现" tabindex="-1"><a class="header-anchor" href="#注意力层及实现"><span>注意力层及实现</span></a></h3><p>自注意力（Self-Attention）是Transformer模型的核心组件，其核心价值在于动态建立序列内部的全局依赖关系。在机器翻译任务中：</p><ul><li>编码阶段：通过自注意力捕捉源语言句子中单词间的语法/语义关联（如主谓宾结构）</li><li>解码阶段：建立目标语言生成与源语言上下文的关联（如代词指代消解）</li></ul><p>给定由单词语义嵌入及其位置编码叠加得到的输入表示，为了实现对上下文语义依赖的建模，进一步引入在自注意力机制中涉及到的三个元素：查询（Query），键（Key），值（Value）。 在编码输入序列中每一个单词的表示的过程中，这三个元素用于计算上下文单词所对应的权重得分(通过点积衡量查询与键的相似度)。 直观地说，这些权重反映了在编码当前单词的表示时，对于上下文不同部分所需要的关注程度。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model</span>
<span class="line">        self<span class="token punctuation">.</span>d_k <span class="token operator">=</span> d_model <span class="token operator">//</span> heads</span>
<span class="line">        self<span class="token punctuation">.</span>h <span class="token operator">=</span> heads</span>
<span class="line">        self<span class="token punctuation">.</span>q_linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>v_linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>k_linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">attention</span><span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> d_k<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># 掩盖掉那些为了填补长度增加的单元，使其通过 softmax 计算后为 0</span></span>
<span class="line">        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span></span>
<span class="line">            mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line">            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span></span>
<span class="line">        scores <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">if</span> dropout <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span></span>
<span class="line">            scores <span class="token operator">=</span> dropout<span class="token punctuation">(</span>scores<span class="token punctuation">)</span></span>
<span class="line">        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> v<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">return</span> output</span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        bs <span class="token operator">=</span> q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># 进行线性操作划分为成 h 个头</span></span>
<span class="line">        k <span class="token operator">=</span> self<span class="token punctuation">.</span>k_linear<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span></span>
<span class="line">        q <span class="token operator">=</span> self<span class="token punctuation">.</span>q_linear<span class="token punctuation">(</span>q<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span></span>
<span class="line">        v <span class="token operator">=</span> self<span class="token punctuation">.</span>v_linear<span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># 矩阵转置</span></span>
<span class="line">        k <span class="token operator">=</span> k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></span>
<span class="line">        q <span class="token operator">=</span> q<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></span>
<span class="line">        v <span class="token operator">=</span> v<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># 计算 attention</span></span>
<span class="line">        scores <span class="token operator">=</span> attention<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># 连接多个头并输入到最后的线性层</span></span>
<span class="line">        concat <span class="token operator">=</span> scores<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span></span>
<span class="line">        output <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>concat<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">return</span> output</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="前馈层及实现" tabindex="-1"><a class="header-anchor" href="#前馈层及实现"><span>前馈层及实现</span></a></h3><p>前馈层接收自注意力子层的输出作为输入，并且通过一个带有Relu激活函数的两层全连接网络对输入进行更复杂的非线性变换。</p><p>试验结果表明，增大前馈子层隐状态的维度有利于提升最终翻译结果的质量，因此，前馈子层隐状态的维度一般要比自注意力子层要大。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">class</span> <span class="token class-name">FeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> d_ff<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># d_ff 默认设置为 2048</span></span>
<span class="line">        self<span class="token punctuation">.</span>linear_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>linear_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="残差连接与层归一化实现" tabindex="-1"><a class="header-anchor" href="#残差连接与层归一化实现"><span>残差连接与层归一化实现</span></a></h3><p>由 Transformer 结构组成的网络结构通常都是非常庞大。编码器和解码器均由很多层基本的Transformer块组成，每一层当中都包含复杂的非线性映射，这就导致模型的训练比较困难。 因此，研究者们在 Transformer 块中进一步引入了残差连接与层归一化技术以进一步提升训练的稳定性。 具体来说，残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出上去，从而避免由于网络过深在优化过程中潜在的梯度消失问题。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">class</span> <span class="token class-name">NormLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>size <span class="token operator">=</span> d_model</span>
<span class="line">        <span class="token comment"># 层归一化包含两个可以学习的参数</span></span>
<span class="line">        self<span class="token punctuation">.</span>alpha <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>self<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> eps</span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        norm <span class="token operator">=</span> self<span class="token punctuation">.</span>alpha <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> \</span>
<span class="line">        <span class="token operator">/</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>std<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>bias</span>
<span class="line">        <span class="token keyword">return</span> norm</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="编码器和解码器结构及实现" tabindex="-1"><a class="header-anchor" href="#编码器和解码器结构及实现"><span>编码器和解码器结构及实现</span></a></h3><p>基于上述模块，根据transformer的网络架构，编码器端可以较为容易实现，只需要牢牢掌握自注意力机制和多头自注意力机制就可以了.</p><p>相比于编码器端，解码器端要更复杂一些。具体来说，解码器的每个Transformer块的第一个自注意力子层额外增加了注意力掩码，对应图中的掩码多头注意力（Masked Multi-Head Attention）部分.</p><p>因为在翻译的过程中，编码器端主要用于编码源语言序列的信息，而这个序列是完全已知的，因而编码器仅需要考虑如何融合上下文语义信息即可。</p><p>而解码端则负责生成目标语言序列，这一生成过程是自回归的，即对于每一个单词的生成过程，仅有当前单词之前的目标语言序列是可以被观测的， 因此这一额外增加的掩码是用来掩盖后续的文本信息，以防模型在训练阶段直接看到后续的文本序列进而无法得到有效地训练。</p><blockquote><p>换句话说，在Transformer推理时，是一个词一个词的输出，但在训练时这样做效率太低了，所以我们会将target一次性给到Transformer，通过对target进行掩码，防止其看到后面的信息，效果等同于一个一个词给解码器。</p></blockquote><p>解码器端还额外增加了一个多头注意力（Multi-Head Attention）模块，使用交叉注意力（Cross-attention）方法，同时接收来自编码器端的输出以及当前Transformer块的前一个掩码注意力层的输出。 查询是通过解码器前一层的输出进行投影的，而键和值是使用编码器的输出进行投影的。它的作用是在翻译的过程当中，为了生成合理的目标语言序列需要观测待翻译的源语言序列是什么。 基于上述的编码器和解码器结构，待翻译的源语言文本，首先经过编码器端的每个Transformer块对其上下文语义的层层抽象，最终输出每一个源语言单词上下文相关的表示。 解码器端以自回归的方式生成目标语言文本，即在每个时间步 t，根据编码器端输出的源语言文本表示，以及前t − 1个时刻生成的目标语言文本，生成当前时刻的目标语言单词。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py" data-title="py"><pre><code><span class="line"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>norm_1 <span class="token operator">=</span> Norm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>norm_2 <span class="token operator">=</span> Norm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>heads<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>ff <span class="token operator">=</span> FeedForward<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>dropout_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>dropout_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout_1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>x2<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout_2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ff<span class="token punctuation">(</span>x2<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">return</span> x</span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> N<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>N <span class="token operator">=</span> N</span>
<span class="line">        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> Embedder<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>pe <span class="token operator">=</span> PositionalEncoder<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> get_clones<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> Norm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">(</span>src<span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pe<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">            x <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>norm_1 <span class="token operator">=</span> Norm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>norm_2 <span class="token operator">=</span> Norm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>norm_3 <span class="token operator">=</span> Norm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>dropout_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>dropout_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>dropout_3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>attn_1 <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>heads<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>attn_2 <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>heads<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>ff <span class="token operator">=</span> FeedForward<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> e_outputs<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> trg_mask<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout_1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn_1<span class="token punctuation">(</span>x2<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> trg_mask<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout_2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn_2<span class="token punctuation">(</span>x2<span class="token punctuation">,</span> e_outputs<span class="token punctuation">,</span> e_outputs<span class="token punctuation">,</span> \</span>
<span class="line">         src_mask<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        x2 <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_3<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout_3<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ff<span class="token punctuation">(</span>x2<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">return</span> x</span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> N<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>N <span class="token operator">=</span> N</span>
<span class="line">        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> Embedder<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>pe <span class="token operator">=</span> PositionalEncoder<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> get_clones<span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> Norm<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> trg<span class="token punctuation">,</span> e_outputs<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> trg_mask<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">(</span>trg<span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pe<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">            x <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> e_outputs<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> trg_mask<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="multi-head-attention-结构" tabindex="-1"><a class="header-anchor" href="#multi-head-attention-结构"><span>Multi-Head Attention 结构</span></a></h2><p>Encoder 和 Decoder 结构中公共的 layer 之一是 Multi-Head Attention，其是由多个 Self-Attention 并行组成的。 Encoder block 只包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。 <img src="/assets/img_15-CeIIRYXc.png" alt="img_15.png"></p><p>Multi-Head Attention (MHA) 是基于 Self-Attention (SA) 的一种变体。MHA 在 SA 的基础上引入了“多头”机制， 将输入拆分为多个子空间，每个子空间分别执行 SA，最后将多个子空间的输出拼接在一起并进行线性变换，从而得到最终的输出。</p><p>对于 MHA，之所以需要对 Q、K、V 进行多头（head）划分，其目的是为了增强模型对不同信息的关注。具体来说，多组 Q、K、V 分别计算 Self-Attention， 每个头自然就会有独立的 Q、K、V 参数，从而让模型同时关注多个不同的信息，这有些类似 CNN 架构模型的多通道机制。 <img src="/assets/img_16-OAtJ6L-3.png" alt="img_16.png"></p><blockquote><p><strong>注意</strong>：MultiHead的head不管有几个，参数量都是一样的。并不是head多，参数就多。 <strong>将输入拆分为多个子空间</strong>，如果输入的维度已经固定了，拆成多少个头，参数都是一样的。MultiHead按照“词向量维度”这个方向，将Q,K,V拆成了多个头。但是head数并不是越多越好。 <img src="/assets/img_10-DBLu53QP.png" alt="img_10.png"></p></blockquote><h2 id="self-attention的细节" tabindex="-1"><a class="header-anchor" href="#self-attention的细节"><span>Self-Attention的细节</span></a></h2><p>在 Self-Attention 中，Q、K、V 是在同一个输入（比如序列中的一个单词）上计算得到的三个向量。 具体来说，我们可以通过对原始输入词的 embedding 进行线性变换（比如使用一个全连接层），来得到 Q、K、V。这三个向量的维度通常都是一样的，取决于模型设计时的决策。</p><p>attention可以有很多种计算方式：加性attention、点积attention，还有带参数的计算方式。 $$\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V$$</p><p>在计算 Self-Attention 时，Q、K、V 被用来计算注意力分数，即用于表示当前位置和其他位置之间的关系。 注意力分数可以通过 Q 和 K 的点积来计算，然后将分数除以 8，再经过一个 softmax 归一化处理，得到每个位置的权重。 然后用这些权重来加权计算 V 的加权和，即得到当前位置的输出。在论文中，输入给 Self-Attention 层的 Q、K、V 的向量维度是 64 ，Embedding Vector 和 Encoder-Decoder 模块输入输出的维度都是 512。</p><blockquote><p>将分数除以 8 的操作，对应图中的 Scale 层，这个参数 8 是 K 向量维度 64 的平方根结果。</p></blockquote><blockquote><p><strong>为什么有缩放因子$\frac{1}{\sqrt{d_k}}$</strong></p><p>缩放因子的作用是「归一化」。</p><p>假设 Q，K 里的元素的均值为 0，方差为 1，那么$A^T = Q^T K$中元素的均值为 0，方差为 d。 当 d 变得很大时，A 中的元素的方差也会变得很大，如果 A 中的元素方差很大，那么 softmax (A) 的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。</p><p>总结一下就是 softmax (A) 的分布会和 d 有关。因此 A 中每一个元素乘上$\frac{1}{\sqrt{d_k}}$后，方差又变为 1。这使得 softmax (A) 的分布“陡峭”程度与 d 解耦，从而使得训练过程中梯度值保持稳定。</p></blockquote><h2 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span>参考文献</span></a></h2><ol><li><a href="https://zhuanlan.zhihu.com/p/657456977" target="_blank" rel="noopener noreferrer">万字长文说明白transformer<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://zhuanlan.zhihu.com/p/163511181" target="_blank" rel="noopener noreferrer">如何训练你的BERT<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://zhuanlan.zhihu.com/p/344352677" target="_blank" rel="noopener noreferrer">一文带你学会 Attention<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://zhuanlan.zhihu.com/p/667635031" target="_blank" rel="noopener noreferrer">transformer结构-输入编码(BPE,PE)剖析<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://zhuanlan.zhihu.com/p/115474279" target="_blank" rel="noopener noreferrer">transformer 的结构 <span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li></ol></div></div><footer class="page-meta"><!----><div class="meta-item last-updated"><span class="xicon-container left meta-item-label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:20px;height:20px;font-size:20px;color:;"><path d="M26 4h-4V2h-2v2h-8V2h-2v2H6c-1.1 0-2 .9-2 2v20c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 22H6V12h20v14zm0-16H6V6h4v2h2V6h8v2h2V6h4v4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->最近更新 2025/8/13 21:41:44<!--]--></span></span></div></footer><!----><div class="reco-valine-wrapper"><div id="valine"></div></div></div><div class="page-catalog-container"><h5 class="tip">ON THIS PAGE</h5><ul><!--[--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blogs/AI/transformer.html#输入编码-bpe-pe-剖析" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="输入编码(BPE,PE)剖析"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->输入编码(BPE,PE)剖析<!--]--></span></span><!--[--><!--]--></a></li><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blogs/AI/transformer.html#bpe分词算法" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="BPE分词算法"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->BPE分词算法<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blogs/AI/transformer.html#pe位置编码" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="PE位置编码"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->PE位置编码<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blogs/AI/transformer.html#input" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="Input"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Input<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--]--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blogs/AI/transformer.html#transformer网络结构" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="Transformer网络结构"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Transformer网络结构<!--]--></span></span><!--[--><!--]--></a></li><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blogs/AI/transformer.html#嵌入表示层以及python实现" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="嵌入表示层以及Python实现"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->嵌入表示层以及Python实现<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blogs/AI/transformer.html#注意力层及实现" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="注意力层及实现"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->注意力层及实现<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blogs/AI/transformer.html#前馈层及实现" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="前馈层及实现"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->前馈层及实现<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blogs/AI/transformer.html#残差连接与层归一化实现" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="残差连接与层归一化实现"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->残差连接与层归一化实现<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_3"><a aria-current="page" href="/blogs/AI/transformer.html#编码器和解码器结构及实现" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="编码器和解码器结构及实现"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->编码器和解码器结构及实现<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--]--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blogs/AI/transformer.html#multi-head-attention-结构" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="Multi-Head Attention 结构"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Multi-Head Attention 结构<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blogs/AI/transformer.html#self-attention的细节" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="Self-Attention的细节"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Self-Attention的细节<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blogs/AI/transformer.html#参考文献" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="参考文献"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->参考文献<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--]--></ul></div></main><!--]--></div></div><!--[--><!----><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-ByLG9LQz.js" defer></script>
  </body>
</html>
